import streamlit as st
import feedparser
import pandas as pd
import datetime
import os
import urllib.parse

# --- åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ ---
def analyze_growth_company(title, summary):
    text = (title + summary).lower()
    growth_keywords = ["æ¡ç”¨", "å‹Ÿé›†", "ç§»è»¢", "å¢—åºŠ", "æ–°æ‹ ç‚¹", "æµ·å¤–å±•é–‹", "æ–°è¦äº‹æ¥­", "è³‡é‡‘èª¿é”", "ææº", "å°å…¥", "é–‹å§‹", "ãƒ­ãƒ¼ãƒ³ãƒ", "å­ä¼šç¤¾"]
    biz_keywords = ["æ³•äºº", "ä¼æ¥­", "b2b", "saas", "dx", "ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³", "oem", "å¸", "åŠ ç›Ÿ", "fc", "ã‚³ãƒ³ã‚µãƒ«"]
    return any(k in text for k in growth_keywords) and any(k in text for k in biz_keywords)

def fetch_all_sources():
    feeds = [
        "https://prtimes.jp/index.rdf",
        "https://news.google.com/rss/search?q=" + urllib.parse.quote("æ¡ç”¨å¼·åŒ– ä¼æ¥­") + "&hl=ja&gl=JP&ceid=JP:ja",
        "https://news.google.com/rss/search?q=" + urllib.parse.quote("æ–°è¦äº‹æ¥­ é–‹å§‹") + "&hl=ja&gl=JP&ceid=JP:ja"
    ]
    
    now = datetime.datetime.now()
    today_str = now.strftime("%Y-%m-%d")
    time_str = now.strftime("%H:%M")
    new_entries = []
    
    for url in feeds:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            if analyze_growth_company(entry.title, entry.summary):
                pub_date = today_str
                if hasattr(entry, 'published'):
                    try: pub_date = pd.to_datetime(entry.published).strftime("%Y-%m-%d")
                    except: pass
                
                # ç°¡æ˜“çš„ã«ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ç¤¾åã‚’æ¨æ¸¬ï¼ˆã€Œæ ªå¼ä¼šç¤¾ã€‡ã€‡ãŒã€œã€ã®å½¢å¼ãŒå¤šã„æƒ³å®šï¼‰
                company = entry.title.split("ã€")[0].split("ã€‘")[0].split("ãŒ")[0].strip()
                new_entries.append([pub_date, time_str, company, entry.title, entry.link])
    
    db_file = "news_database.csv"
    if new_entries:
        df_new = pd.DataFrame(new_entries, columns=["date", "time", "company", "title", "url"])
        if os.path.exists(db_file):
            df_old = pd.read_csv(db_file)
            # æ—¢å­˜ãƒªã‚¹ãƒˆã«ãªã„ä¼šç¤¾åã‚’ã€Œæ–°è¦ã€ã¨ã—ã¦ãƒãƒ¼ã‚¯ã™ã‚‹ãŸã‚ã®æº–å‚™
            existing_companies = set(df_old["company"].unique())
            df_final = pd.concat([df_new, df_old]).drop_duplicates(subset=["url"], keep="first")
        else:
            existing_companies = set()
            df_final = df_new
        
        df_final = df_final.sort_values(by=["date", "time"], ascending=False)
        df_final.to_csv(db_file, index=False, encoding="utf_8_sig")
        return len(new_entries), existing_companies
    return 0, set()

# --- Streamlit ãƒ‡ã‚¶ã‚¤ãƒ³è¨­å®š ---
st.set_page_config(page_title="Growth Company Hub", layout="wide")

# ã‚«ã‚¹ã‚¿ãƒ CSSã§è¦‹ãŸç›®ã‚’æ•´ãˆã‚‹
st.markdown("""
    <style>
    .reportview-container { background: #f0f2f6; }
    .stCard { border: 1px solid #e6e9ef; padding: 15px; border-radius: 10px; background: white; margin-bottom: 10px; box-shadow: 2px 2px 5px rgba(0,0,0,0.05); }
    .new-label { background-color: #ff4b4b; color: white; padding: 2px 8px; border-radius: 4px; font-size: 12px; font-weight: bold; margin-right: 8px; }
    .tag { background-color: #007bff; color: white; padding: 2px 8px; border-radius: 4px; font-size: 12px; margin-right: 5px; }
    </style>
    """, unsafe_allow_html=True)

st.title("ğŸš€ æˆé•·ä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³")
st.caption("PR TIMESãƒ»Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰æˆé•·æ„æ¬²ã®é«˜ã„toBä¼æ¥­ã‚’è‡ªå‹•æŠ½å‡ºã—ã¦ã„ã¾ã™")

db_file = "news_database.csv"

# æ›´æ–°ãƒœã‚¿ãƒ³
if st.button("ğŸ”„ æœ€æ–°æƒ…å ±ã‚’å–å¾—ã—ã¦æ›´æ–°"):
    with st.spinner("ã‚¹ã‚­ãƒ£ãƒ‹ãƒ³ã‚°ä¸­..."):
        count, _ = fetch_all_sources()
        st.success(f"{count}ä»¶ã®è¨˜äº‹ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚")
        st.rerun()

st.divider()

if os.path.exists(db_file):
    df = pd.read_csv(db_file)
    
    # éå»ã®ä¼šç¤¾ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆNEWãƒ©ãƒ™ãƒ«åˆ¤å®šç”¨ï¼‰
    # ç¾åœ¨è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ä¸­ã§ã€ãã‚Œä»¥å‰ã®ãƒ‡ãƒ¼ã‚¿ã«åå‰ãŒãªã„ã‚‚ã®ã‚’åˆ¤å®š
    dates = df["date"].unique()
    
    for d in dates:
        st.markdown(f"### ğŸ“… {d}")
        day_df = df[df["date"] == d]
        
        for idx, row in day_df.iterrows():
            # NEWãƒ©ãƒ™ãƒ«åˆ¤å®šï¼šã“ã®ä¼šç¤¾ãŒã“ã‚Œä»¥å‰ï¼ˆå¤ã„æ—¥ä»˜ï¼‰ã®ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹ã‹
            past_data = df[df["date"] < d]
            is_new = row['company'] not in past_data['company'].values if not past_data.empty else True
            
            # ã‚¿ã‚°ç”Ÿæˆ
            tags = []
            if "æ¡ç”¨" in str(row['title']): tags.append("ğŸ”¥æ¡ç”¨å¼·åŒ–")
            if "è³‡é‡‘" in str(row['title']): tags.append("ğŸ’°è³‡é‡‘èª¿é”")
            if "ç§»è»¢" in str(row['title']) or "æ‹ ç‚¹" in str(row['title']): tags.append("ğŸ“æ‹ ç‚¹æ‹¡å¤§")
            if "æ–°ã‚µãƒ¼ãƒ“ã‚¹" in str(row['title']) or "é–‹å§‹" in str(row['title']): tags.append("ğŸš€æ–°äº‹æ¥­")

            # HTMLã§ã‚«ãƒ¼ãƒ‰é¢¨ã®è¦‹ãŸç›®ã‚’ä½œæˆ
            new_badge = '<span class="new-label">NEW</span>' if is_new else ""
            tag_html = "".join([f'<span class="tag">{t}</span>' for t in tags])
            
            with st.container():
                st.markdown(f"""
                <div class="stCard">
                    <small>{row['time']} | {row['company']}</small>< pybr>
                    {new_badge}<strong><a href="{row['url']}" target="_blank" style="text-decoration: none; color: #1f77b4;">{row['title']}</a></strong><br>
                    <div style="margin-top: 8px;">{tag_html}</div>
                </div>
                """, unsafe_allow_html=True)
else:
    st.info("ã€Œæœ€æ–°æƒ…å ±ã‚’å–å¾—ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ã‚¹ã‚­ãƒ£ãƒ³ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")
