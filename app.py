import streamlit as st
import feedparser
import pandas as pd
import datetime
import os
import urllib.parse

# --- åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼šæˆé•·æ„æ¬²ãƒ»toBåˆ¤å®š ---
def analyze_growth_company(title, summary):
    text = (title + summary).lower()
    growth_keywords = ["æ¡ç”¨", "å‹Ÿé›†", "ç§»è»¢", "å¢—åºŠ", "æ–°æ‹ ç‚¹", "æµ·å¤–å±•é–‹", "æ–°è¦äº‹æ¥­", "è³‡é‡‘èª¿é”", "ææº", "å°å…¥", "é–‹å§‹", "ãƒ­ãƒ¼ãƒ³ãƒ", "å­ä¼šç¤¾", "æ‹ ç‚¹ã‚’æ–°è¨­"]
    biz_keywords = ["æ³•äºº", "ä¼æ¥­", "b2b", "saas", "dx", "ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³", "oem", "å¸", "åŠ ç›Ÿ", "fc", "ã‚³ãƒ³ã‚µãƒ«", "ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ "]
    return any(k in text for k in growth_keywords) and any(k in text for k in biz_keywords)

def fetch_all_sources():
    feeds = [
        "https://prtimes.jp/index.rdf",
        "https://news.google.com/rss/search?q=" + urllib.parse.quote("æ¡ç”¨å¼·åŒ– ä¼æ¥­") + "&hl=ja&gl=JP&ceid=JP:ja",
        "https://news.google.com/rss/search?q=" + urllib.parse.quote("æ–°æ‹ ç‚¹ è¨­ç«‹") + "&hl=ja&gl=JP&ceid=JP:ja",
        "https://news.google.com/rss/search?q=" + urllib.parse.quote("æ–°è¦äº‹æ¥­ é–‹å§‹") + "&hl=ja&gl=JP&ceid=JP:ja"
    ]
    
    now = datetime.datetime.now()
    today_str = now.strftime("%Y-%m-%d")
    time_str = now.strftime("%H:%M")
    new_entries = []
    
    for url in feeds:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            if analyze_growth_company(entry.title, entry.summary):
                # ç¤¾åã®ç°¡æ˜“æŠ½å‡º
                title_clean = entry.title.replace("ã€", " ").replace("ã€‘", " ").replace("ã€Œ", " ").replace("ã€", " ")
                company = title_clean.split("ãŒ")[0].split("ã®")[0].strip()[:20]
                
                pub_date = today_str
                if hasattr(entry, 'published'):
                    try: pub_date = pd.to_datetime(entry.published).strftime("%Y-%m-%d")
                    except: pass
                
                new_entries.append([pub_date, time_str, company, entry.title, entry.link])
    
    db_file = "news_database.csv"
    if new_entries:
        df_new = pd.DataFrame(new_entries, columns=["date", "time", "company", "title", "url"])
        if os.path.exists(db_file):
            try:
                df_old = pd.read_csv(db_file)
                # åˆ—ãŒè¶³ã‚Šãªã„å¤ã„ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯æ¨ã¦ã‚‹
                if "company" not in df_old.columns:
                    df_final = df_new
                else:
                    df_final = pd.concat([df_new, df_old]).drop_duplicates(subset=["url"], keep="first")
            except:
                df_final = df_new
        else:
            df_final = df_new
        
        df_final = df_final.sort_values(by=["date", "time"], ascending=False)
        df_final.to_csv(db_file, index=False, encoding="utf_8_sig")
        return len(new_entries)
    return 0

# --- ç”»é¢ãƒ‡ã‚¶ã‚¤ãƒ³ ---
st.set_page_config(page_title="Growth Company Hub", layout="wide")

st.markdown("""
    <style>
    .main { background-color: #f8f9fa; }
    .stCard { border: 1px solid #dee2e6; padding: 20px; border-radius: 12px; background: white; margin-bottom: 15px; box-shadow: 0 4px 6px rgba(0,0,0,0.05); }
    .new-label { background: linear-gradient(45deg, #ff4b4b, #ff8f8f); color: white; padding: 3px 10px; border-radius: 20px; font-size: 11px; font-weight: bold; margin-right: 10px; }
    .tag { background-color: #e9ecef; color: #495057; padding: 3px 10px; border-radius: 5px; font-size: 11px; margin-right: 5px; border: 1px solid #ced4da; }
    .company-name { color: #6c757d; font-size: 13px; font-weight: 600; margin-bottom: 5px; }
    .title-link { color: #1f77b4; font-size: 18px; font-weight: bold; text-decoration: none; }
    .title-link:hover { text-decoration: underline; color: #125688; }
    </style>
    """, unsafe_allow_html=True)

st.title("ğŸš€ æˆé•·ä¼æ¥­ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ»ãƒªã‚¹ãƒˆ")
st.caption("æœ€æ–°é †ã«ä¸¦ã‚“ã§ã„ã¾ã™ã€‚åˆã‚ã¦æ¤œå‡ºã•ã‚ŒãŸä¼æ¥­ã«ã¯ NEW ãƒ©ãƒ™ãƒ«ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

db_file = "news_database.csv"

if st.button("ğŸ”„ æœ€æ–°æƒ…å ±ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦æ›´æ–°"):
    with st.spinner("æƒ…å ±ã‚’é›†ã‚ã¦ã„ã¾ã™..."):
        count = fetch_all_sources()
        st.success(f"{count} ä»¶ã®æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åæ˜ ã—ã¾ã—ãŸã€‚")
        st.rerun()

st.divider()

if os.path.exists(db_file):
    df = pd.read_csv(db_file)
    if not df.empty:
        # æ—¥ä»˜ãƒªã‚¹ãƒˆã‚’é™é †ã§
        dates = df["date"].unique()
        
        for d in dates:
            st.markdown(f"#### ğŸ“… {d}")
            day_df = df[df["date"] == d]
            
            for _, row in day_df.iterrows():
                # NEWåˆ¤å®šï¼šéå»ã®æ—¥ä»˜ï¼ˆè‡ªèº«ã‚ˆã‚Šå¤ã„æ—¥ä»˜ï¼‰ã«ãã®ç¤¾åãŒã‚ã‚‹ã‹
                past_data = df[df["date"] < row['date']]
                is_new = row['company'] not in past_data['company'].values if not past_data.empty else True
                
                # ã‚¿ã‚°
                tags = []
                if "æ¡ç”¨" in str(row['title']): tags.append("ğŸ”¥ æ¡ç”¨å¼·åŒ–")
                if "è³‡é‡‘" in str(row['title']): tags.append("ğŸ’° è³‡é‡‘èª¿é”")
                if "æ‹ ç‚¹" in str(row['title']) or "ç§»è»¢" in str(row['title']): tags.append("ğŸ“ æ‹ ç‚¹æ‹¡å¤§")
                if "äº‹æ¥­" in str(row['title']) or "é–‹å§‹" in str(row['title']): tags.append("ğŸš€ æ–°äº‹æ¥­")
                
                new_badge = '<span class="new-label">NEW</span>' if is_new else ""
                tag_html = "".join([f'<span class="tag">{t}</span>' for t in tags])
                
                st.markdown(f"""
                <div class="stCard">
                    <div class="company-name">{row['time']} | {row['company']}</div>
                    {new_badge}<a class="title-link" href="{row['url']}" target="_blank">{row['title']}</a>
                    <div style="margin-top: 10px;">{tag_html}</div>
                </div>
                """, unsafe_allow_html=True)
    else:
        st.info("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚æ›´æ–°ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
else:
    st.info("ã€Œæœ€æ–°æƒ…å ±ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦æ›´æ–°ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨ãƒªã‚¹ãƒˆãŒä½œæˆã•ã‚Œã¾ã™ã€‚")
