import streamlit as st
import feedparser
import pandas as pd
import datetime
import os
import urllib.parse

# --- åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼šæˆé•·æ„æ¬²ã®ã‚ã‚‹toBä¼æ¥­ã‚’æŠ½å‡º ---
def analyze_growth_company(title, summary):
    text = (title + summary).lower()
    # æˆé•·æ„æ¬²ã‚’ç¤ºã™ãƒ¯ãƒ¼ãƒ‰
    growth_keywords = ["æ¡ç”¨", "å‹Ÿé›†", "ç§»è»¢", "å¢—åºŠ", "æ–°æ‹ ç‚¹", "æµ·å¤–å±•é–‹", "æ–°è¦äº‹æ¥­", "è³‡é‡‘èª¿é”", "ææº", "å°å…¥", "é–‹å§‹", "ãƒ­ãƒ¼ãƒ³ãƒ"]
    # ãƒ“ã‚¸ãƒã‚¹ãƒ¯ãƒ¼ãƒ‰
    biz_keywords = ["æ³•äºº", "ä¼æ¥­", "b2b", "saas", "dx", "ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³", "oem", "å¸", "åŠ ç›Ÿ", "fc"]

    is_growth = any(k in text for k in growth_keywords)
    is_biz = any(k in text for k in biz_keywords)
    return is_growth and is_biz

def fetch_all_sources():
    feeds = [
        "https://prtimes.jp/index.rdf", # PR TIMES ç·åˆ
        "https://news.google.com/rss/search?q=" + urllib.parse.quote("æ¡ç”¨å¼·åŒ– ä¼æ¥­") + "&hl=ja&gl=JP&ceid=JP:ja",
        "https://news.google.com/rss/search?q=" + urllib.parse.quote("æ–°è¦äº‹æ¥­ é–‹å§‹") + "&hl=ja&gl=JP&ceid=JP:ja"
    ]
    
    now = datetime.datetime.now()
    today_str = now.strftime("%Y-%m-%d")
    time_str = now.strftime("%H:%M") # å–å¾—æ™‚åˆ»ã‚‚è¨˜éŒ²
    new_data = []
    
    for url in feeds:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            if analyze_growth_company(entry.title, entry.summary):
                pub_date = today_str
                if hasattr(entry, 'published'):
                    try:
                        pub_date = pd.to_datetime(entry.published).strftime("%Y-%m-%d")
                    except: pass
                
                # [æ—¥ä»˜, æ™‚åˆ», ã‚¿ã‚¤ãƒˆãƒ«, URL] ã®å½¢å¼ã§æ ¼ç´
                new_data.append([pub_date, time_str, entry.title, entry.link])
    
    db_file = "news_database.csv"
    if new_data:
        df_new = pd.DataFrame(new_data, columns=["date", "time", "title", "url"])
        if os.path.exists(db_file):
            df_old = pd.read_csv(db_file)
            # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œä¸Šã€ã«ã—ã¦çµåˆã—ã€é‡è¤‡ã‚’æ’é™¤
            df_final = pd.concat([df_new, df_old]).drop_duplicates(subset=["url"], keep="first")
        else:
            df_final = df_new
        
        # æ—¥ä»˜ã¨æ™‚åˆ»ã§æœ€æ–°é †ã«ä¸¦ã³æ›¿ãˆ
        df_final = df_final.sort_values(by=["date", "time"], ascending=False)
        df_final.to_csv(db_file, index=False, encoding="utf_8_sig")
        return len(new_data)
    return 0

# --- Streamlit UI ---
st.set_page_config(page_title="æˆé•·ä¼æ¥­ã‚­ãƒ£ãƒƒãƒãƒ£ãƒ¼", layout="wide")
st.title("ğŸš€ æœ€æ–°ï¼šæˆé•·æ„æ¬²ã®ã‚ã‚‹ä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹")

db_file = "news_database.csv"

# è‡ªå‹•æ›´æ–°ãƒœã‚¿ãƒ³
if st.button("ğŸ”„ æœ€æ–°æƒ…å ±ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ä¸€ç•ªä¸Šã«è¿½åŠ "):
    with st.spinner("ã‚¹ã‚­ãƒ£ãƒ³ä¸­..."):
        count = fetch_all_sources()
        st.success(f"{count}ä»¶ã®æ–°ã—ã„å‹•ãã‚’æ¤œçŸ¥ã—ã¾ã—ãŸã€‚")
        st.rerun()

st.markdown("---")

if os.path.exists(db_file):
    df = pd.read_csv(db_file)
    
    # æ—¥ä»˜ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦è¡¨ç¤º
    dates = df["date"].unique()
    
    for d in dates:
        st.subheader(f"ğŸ“… {d}")
        day_df = df[df["date"] == d]
        
        for _, row in day_df.iterrows():
            # ãƒ©ãƒ™ãƒ«ä»˜ã‘
            tags = ""
            if "æ¡ç”¨" in str(row['title']): tags += " ğŸ”¥æ¡ç”¨"
            if "è³‡é‡‘" in str(row['title']): tags += " ğŸ’°èª¿é”"
            if "ç§»è»¢" in str(row['title']) or "æ‹ ç‚¹" in str(row['title']): tags += " ğŸ“æ‹¡å¤§"
            
            # å–å¾—æ™‚åˆ»ã‚’è¡¨ç¤ºã™ã‚‹ã“ã¨ã§ã€Œç©ã¿ä¸ŠãŒã£ã¦ã„ã‚‹æ„Ÿã€ã‚’å‡ºã™
            time_prefix = f"[{row['time']}] " if 'time' in df.columns else ""
            st.markdown(f"{time_prefix}**{tags}** [{row['title']}]({row['url']})")
else:
    st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã€Œæœ€æ–°æƒ…å ±ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
