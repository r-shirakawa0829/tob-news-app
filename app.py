import streamlit as st
import feedparser
import pandas as pd
import datetime
import os

# --- åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå‰å›ã®å¼·åŒ–ç‰ˆã‚’ç¶­æŒï¼‰ ---
def is_tob_news(title, summary):
    text = (title + summary).lower()
    biz_keywords = ["fc", "ãƒ•ãƒ©ãƒ³ãƒãƒ£ã‚¤ã‚º", "åŠ ç›Ÿåº—", "å¸", "æ¥­å‹™ç”¨", "æ³•äººå‘ã‘", "oem", "dx", "saas", "åº—èˆ—é–‹ç™º", "ç¦åˆ©åšç”Ÿ", "ã‚ªãƒ•ã‚£ã‚¹ç”¨"]
    if any(k in text for k in biz_keywords): return True
    consumer_keywords = ["æ–°ç™ºå£²", "æœŸé–“é™å®š", "é£Ÿã¹æ”¾é¡Œ", "å®Ÿé£Ÿãƒ¬ãƒ", "å…¬å¼sns"]
    if any(k in title for k in consumer_keywords): return False
    base_tob = ["ææº", "å°å…¥", "é–‹å§‹", "æ”¯æ´", "ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³", "é–‹ç™º", "èª¿é”", "è¨­ç«‹"]
    return any(k in text for k in base_tob)

def fetch_news():
    urls = [
        "https://prtimes.jp/main/html/searchrlp/ctcd/100/f/rss.xml", # ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—
        "https://prtimes.jp/main/html/searchrlp/ctcd/13/f/rss.xml"   # å¤–é£Ÿãƒ»ä¸­å …
    ]
    today = datetime.date.today().strftime("%Y-%m-%d")
    new_data = []
    for url in urls:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            if is_tob_news(entry.title, entry.summary):
                new_data.append([today, entry.title, entry.link])
    
    db_file = "news_database.csv"
    df_new = pd.DataFrame(new_data, columns=["date", "title", "url"])
    if os.path.exists(db_file):
        df_old = pd.read_csv(db_file)
        df_final = pd.concat([df_old, df_new]).drop_duplicates(subset=["url"])
    else:
        df_final = df_new
    df_final.to_csv(db_file, index=False, encoding="utf_8_sig")
    return df_final

# --- ç”»é¢è¡¨ç¤º (Streamlit) ---
st.set_page_config(page_title="toBä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼", layout="wide")
st.title("ğŸ“… toBä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼")

db_file = "news_database.csv"

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šæ‰‹å‹•è¿½åŠ ãƒ»å–å¾—æ©Ÿèƒ½
st.sidebar.header("æ©Ÿèƒ½ãƒ¡ãƒ‹ãƒ¥ãƒ¼")
if st.sidebar.button("æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è‡ªå‹•å–å¾—"):
    fetch_news()
    st.success("å–å¾—ã—ã¾ã—ãŸï¼")
    st.rerun()

st.sidebar.markdown("---")
st.sidebar.subheader("â• ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ‰‹å‹•è¿½åŠ ")
new_date = st.sidebar.date_input("è¿½åŠ ã™ã‚‹æ—¥ä»˜", datetime.date.today())
new_title = st.sidebar.text_input("ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«")
new_url = st.sidebar.text_input("URL (ä»»æ„)")

if st.sidebar.button("ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã«è¿½åŠ "):
    if new_title:
        add_data = pd.DataFrame([[new_date.strftime("%Y-%m-%d"), new_title, new_url]], columns=["date", "title", "url"])
        if os.path.exists(db_file):
            df_old = pd.read_csv(db_file)
            df_final = pd.concat([df_old, add_data]).drop_duplicates()
        else:
            df_final = add_data
        df_final.to_csv(db_file, index=False, encoding="utf_8_sig")
        st.sidebar.success("è¿½åŠ ã—ã¾ã—ãŸï¼")
        st.rerun()
    else:
        st.sidebar.error("ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ï¼šã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ
col1, col2 = st.columns([1, 2])

with col1:
    st.subheader("æ—¥ä»˜ã‚’é¸æŠ")
    # è¦–è¦šçš„ãªã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‚’è¡¨ç¤º
    selected_date = st.date_input("ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¦‹ãŸã„æ—¥ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„", datetime.date.today())
    target_date_str = selected_date.strftime("%Y-%m-%d")

with col2:
    st.subheader(f"ğŸ” {target_date_str} ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹")
    if os.path.exists(db_file):
        df = pd.read_csv(db_file)
        # æ–‡å­—åˆ—ã¨ã—ã¦æ¯”è¼ƒã™ã‚‹ãŸã‚ã«å¤‰æ›
        display_df = df[df["date"] == target_date_str]
        
        if len(display_df) == 0:
            st.info("ã“ã®æ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            for _, row in display_df.iterrows():
                link = row['url'] if pd.notna(row['url']) and row['url'] != "" else "#"
                st.markdown(f"âœ… [{row['title']}]({link})")
    else:
        st.warning("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰å–å¾—ã—ã¦ãã ã•ã„ã€‚")
