import streamlit as st
import feedparser
import pandas as pd
import datetime
import os
import urllib.parse

# --- åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼šæˆé•·æ„æ¬²ã®ã‚ã‚‹toBä¼æ¥­ã‚’æŠ½å‡º ---
def analyze_growth_company(title, summary):
    text = (title + summary).lower()
    
    # æˆé•·æ„æ¬²ã‚’ç¤ºã™ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ¯ãƒ¼ãƒ‰
    growth_keywords = ["æ¡ç”¨", "å‹Ÿé›†", "ç§»è»¢", "å¢—åºŠ", "æ–°æ‹ ç‚¹", "æµ·å¤–å±•é–‹", "æ–°è¦äº‹æ¥­", "è³‡é‡‘èª¿é”", "ææº", "å°å…¥"]
    # æ³•äººå‘ã‘/ãƒ“ã‚¸ãƒã‚¹ãƒ¯ãƒ¼ãƒ‰
    biz_keywords = ["æ³•äºº", "ä¼æ¥­", "b2b", "saas", "dx", "ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³", "oem", "å¸", "åŠ ç›Ÿ", "fc"]

    is_growth = any(k in text for k in growth_keywords)
    is_biz = any(k in text for k in biz_keywords)
    
    return is_growth and is_biz

def fetch_all_sources():
    # å–å¾—ã‚½ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆ
    feeds = [
        "https://prtimes.jp/index.rdf", # PR TIMES ç·åˆ
        "https://prtimes.jp/main/html/searchrlp/ctcd/100/f/rss.xml", # ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—
        # Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰ã€Œæ¡ç”¨ å¼·åŒ–ã€ã€Œæ–°è¦äº‹æ¥­ã€ãªã©ã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã—ãŸçµæœ
        "https://news.google.com/rss/search?q=" + urllib.parse.quote("æ¡ç”¨å¼·åŒ– ä¼æ¥­") + "&hl=ja&gl=JP&ceid=JP:ja",
        "https://news.google.com/rss/search?q=" + urllib.parse.quote("æ–°ã‚µãƒ¼ãƒ“ã‚¹ é–‹å§‹") + "&hl=ja&gl=JP&ceid=JP:ja"
    ]
    
    today = datetime.date.today().strftime("%Y-%m-%d")
    new_data = []
    
    for url in feeds:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            if analyze_growth_company(entry.title, entry.summary):
                # æ—¥ä»˜å–å¾—ï¼ˆGoogleãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å½¢å¼ã«ã‚‚å¯¾å¿œï¼‰
                pub_date = today # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ä»Šæ—¥
                if hasattr(entry, 'published'):
                    try:
                        pub_date = pd.to_datetime(entry.published).strftime("%Y-%m-%d")
                    except: pass
                
                new_data.append([pub_date, entry.title, entry.link])
    
    db_file = "news_database.csv"
    if new_data:
        df_new = pd.DataFrame(new_data, columns=["date", "title", "url"])
        if os.path.exists(db_file):
            df_old = pd.read_csv(db_file)
            df_final = pd.concat([df_old, df_new]).drop_duplicates(subset=["url"])
        else:
            df_final = df_new
        df_final.to_csv(db_file, index=False, encoding="utf_8_sig")
        return len(new_data)
    return 0

# --- Streamlit UI ---
st.set_page_config(page_title="æˆé•·ä¼æ¥­ã‚­ãƒ£ãƒƒãƒãƒ£ãƒ¼", layout="wide")
st.title("ğŸš€ æˆé•·æ„æ¬²ã®ã‚ã‚‹ä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚")

db_file = "news_database.csv"
if st.button("æœ€æ–°æƒ…å ±ã‚’ä¸€æ‹¬ã‚¹ã‚­ãƒ£ãƒ³"):
    with st.spinner("PR TIMESãƒ»Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å·¡å›ä¸­..."):
        count = fetch_all_sources()
        st.success(f"{count}ä»¶ã®æˆé•·ã«é–¢é€£ã™ã‚‹è¨˜äº‹ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")
        st.rerun()

col1, col2 = st.columns([1, 2])
with col1:
    selected_date = st.date_input("æ—¥ä»˜ã‚’é¸æŠ", datetime.date.today())
    target_str = selected_date.strftime("%Y-%m-%d")

with col2:
    st.subheader(f"ğŸ” {target_str} ã®æˆé•·ä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹")
    if os.path.exists(db_file):
        df = pd.read_csv(db_file)
        display_df = df[df["date"] == target_str]
        if not display_df.empty:
            for _, row in display_df.iterrows():
                # ã‚¿ã‚¤ãƒˆãƒ«ã«ç‰¹å®šã®ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Œã°ãƒãƒƒã‚¸ã‚’è¡¨ç¤º
                label = ""
                if "æ¡ç”¨" in row['title']: label = " ğŸ”¥æ¡ç”¨å¼·åŒ–"
                if "è³‡é‡‘èª¿é”" in row['title']: label = " ğŸ’°è³‡é‡‘èª¿é”"
                st.markdown(f"âœ… **{label}** [{row['title']}]({row['url']})")
        else:
            st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒ£ãƒ³ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
