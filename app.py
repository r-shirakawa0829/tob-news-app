import streamlit as st
import feedparser
import pandas as pd
import datetime
import os

# --- åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã®å¼·åŒ– ---
def is_tob_news(title, summary):
    text = (title + summary).lower()
    
    # å¼·åˆ¶toBãƒ¯ãƒ¼ãƒ‰ï¼šã“ã‚ŒãŒã‚ã‚Œã°ã‚¹ã‚¤ãƒ¼ãƒ„ã ã‚ã†ãŒä½•ã ã‚ã†ãŒæ®‹ã™
    biz_keywords = ["fc", "ãƒ•ãƒ©ãƒ³ãƒãƒ£ã‚¤ã‚º", "åŠ ç›Ÿåº—", "å¸", "æ¥­å‹™ç”¨", "æ³•äººå‘ã‘", "oem", "dx", "saas", "åº—èˆ—é–‹ç™º", "ç¦åˆ©åšç”Ÿ", "ã‚ªãƒ•ã‚£ã‚¹ç”¨"]
    if any(k in text for k in biz_keywords):
        return True
    
    # toCé™¤å¤–ãƒ¯ãƒ¼ãƒ‰ï¼šãƒ“ã‚¸ãƒã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒãªã„çŠ¶æ…‹ã§ã“ã‚Œã‚‰ãŒã‚ã‚Œã°é™¤å¤–
    consumer_keywords = ["æ–°ç™ºå£²", "æœŸé–“é™å®š", "é£Ÿã¹æ”¾é¡Œ", "å®Ÿé£Ÿãƒ¬ãƒ", "å…¬å¼sns"]
    if any(k in title for k in consumer_keywords):
        return False

    # ä¸€èˆ¬çš„ãªãƒ“ã‚¸ãƒã‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    base_tob = ["ææº", "å°å…¥", "é–‹å§‹", "æ”¯æ´", "ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³", "é–‹ç™º", "èª¿é”", "è¨­ç«‹"]
    return any(k in text for k in base_tob)

def fetch_news():
    # PR TIMES ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ— & å¤–é£Ÿãƒ»ä¸­å …ã‚«ãƒ†ã‚´ãƒªãªã©ã‹ã‚‰å–å¾—
    urls = [
        "https://prtimes.jp/main/html/searchrlp/ctcd/100/f/rss.xml", # ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—
        "https://prtimes.jp/main/html/searchrlp/ctcd/13/f/rss.xml"   # å¤–é£Ÿãƒ»ä¸­å …
    ]
    
    today = datetime.date.today().strftime("%Y-%m-%d")
    new_data = []
    
    for url in urls:
        feed = feedparser.parse(url)
        for entry in feed.entries:
            if is_tob_news(entry.title, entry.summary):
                new_data.append([today, entry.title, entry.link])
    
    db_file = "news_database.csv"
    df_new = pd.DataFrame(new_data, columns=["date", "title", "url"])
    
    if os.path.exists(db_file):
        df_old = pd.read_csv(db_file)
        df_final = pd.concat([df_old, df_new]).drop_duplicates(subset=["url"])
    else:
        df_final = df_new
        
    df_final.to_csv(db_file, index=False, encoding="utf_8_sig")
    return df_final

# --- ç”»é¢è¡¨ç¤º (Streamlit) ---
st.set_page_config(page_title="toBä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹", layout="wide")
st.title("ğŸ¢ toBä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§")

# åˆå›èµ·å‹•æ™‚ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã«è‡ªå‹•å–å¾—
db_file = "news_database.csv"
if not os.path.exists(db_file):
    with st.spinner("åˆå›ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ä¸­ã§ã™..."):
        fetch_news()

if os.path.exists(db_file):
    df = pd.read_csv(db_file)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    st.sidebar.header("è¡¨ç¤ºè¨­å®š")
    if st.sidebar.button("æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ‰‹å‹•å–å¾—"):
        fetch_news()
        st.rerun()

    date_list = sorted(df["date"].unique(), reverse=True)
    selected_date = st.sidebar.selectbox("è¡¨ç¤ºã™ã‚‹æ—¥ä»˜ã‚’é¸æŠ", date_list)

    # è¨˜äº‹è¡¨ç¤º
    st.subheader(f"ğŸ“… {selected_date} ã®æ³¨ç›®ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹")
    display_df = df[df["date"] == selected_date]
    
    if len(display_df) == 0:
        st.write("ã“ã®æ—¥ã®toBãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        for _, row in display_df.iterrows():
            st.markdown(f"âœ… [{row['title']}]({row['url']})")
else:
    st.warning("ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚ã€Œæ‰‹å‹•å–å¾—ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã‹ã€è‡ªå‹•æ›´æ–°ã‚’å¾…ã£ã¦ãã ã•ã„ã€‚")
